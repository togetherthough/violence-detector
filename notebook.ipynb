{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import einops\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_constant = 42\n",
    "np.random.seed(seed_constant)\n",
    "random.seed(seed_constant)\n",
    "tf.random.set_seed(seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 120\n",
    "NUM_FRAMES = 10\n",
    "INPUT_SHAPE = (100, 18, 18, 3) \n",
    "INPUT_SHAPE_2D = (18, 18, 3)\n",
    "MAX_FRAMES = 100\n",
    "HEIGHT = 18\n",
    "WIDTH = 18\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 30\n",
    "DATA_NUM = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dtype\n",
    "\n",
    "\n",
    "def load_video(path, max_frames= MAX_FRAMES, resize = (18,18)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        frame_count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame_count >= max_frames:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frames.append(frame.astype(dtype))\n",
    "            frame_count += 1\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        \n",
    "    while len(frames) < max_frames:\n",
    "        frames.append(frames[-1].astype(dtype))\n",
    "\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(folder_path):\n",
    "    class_names = [\"NonViolence\", \"Violence\"]\n",
    "    x, y = [], []\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        class_folder = os.path.join(folder_path, class_name)\n",
    "        video_cnt = 0\n",
    "        for video_file in os.listdir(class_folder):\n",
    "            if video_cnt > DATA_NUM:\n",
    "                break\n",
    "            video_path = os.path.join(class_folder, video_file)\n",
    "            frames = load_video(video_path)\n",
    "            x.append(frames)\n",
    "            y.append(class_index)\n",
    "            video_cnt += 1\n",
    "\n",
    "    return np.array(x), keras.utils.to_categorical(y, num_classes=len(class_names))\n",
    "\n",
    "dataset_path = \"D:\\detect violence\"\n",
    "\n",
    "x, y = prepare_dataset(dataset_path)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.1, random_state = 123)\n",
    "\n",
    "X_train = X_train[:50]\n",
    "Y_train = Y_train[:50]\n",
    "X_test = X_test[:20]\n",
    "Y_test = Y_test[:20]\n",
    "#Y_train = keras.utils.to_categorical(Y_train, num_classes=2)\n",
    "#Y_test = keras.utils.to_categorical(Y_test, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the layers for the 3D Convolutional Neural Network\n",
    "\n",
    "class Conv2Plus1D(keras.layers.Layer):\n",
    "  def __init__(self, filters, kernel_size, padding):\n",
    "    \"\"\"\n",
    "      A sequence of convolutional layers that first apply the convolution operation over the\n",
    "      spatial dimensions, and then the temporal dimension. \n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([  \n",
    "        # Spatial decomposition\n",
    "        layers.Conv3D(filters=filters,\n",
    "                      kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                      padding=padding),\n",
    "        # Temporal decomposition\n",
    "        layers.Conv3D(filters=filters, \n",
    "                      kernel_size=(kernel_size[0], 1, 1),\n",
    "                      padding=padding)\n",
    "        ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)\n",
    "  \n",
    "\n",
    "class ResidualMain(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Residual block of the model with convolution, layer normalization, and the\n",
    "    activation function, ReLU.\n",
    "  \"\"\"\n",
    "  def __init__(self, filters, kernel_size):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        Conv2Plus1D(filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.ReLU(),\n",
    "        Conv2Plus1D(filters=filters, \n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)\n",
    "  \n",
    "class Project(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Project certain dimensions of the tensor as the data is passed through different \n",
    "    sized filters and downsampled. \n",
    "  \"\"\"\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        layers.Dense(units),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)\n",
    "\n",
    "def add_residual_block(input, filters, kernel_size):\n",
    "  \"\"\"\n",
    "    Add residual blocks to the model. If the last dimensions of the input data\n",
    "    and filter size does not match, project it such that last dimension matches.\n",
    "  \"\"\"\n",
    "  out = ResidualMain(filters, \n",
    "                     kernel_size)(input)\n",
    "\n",
    "  res = input\n",
    "  # Using the Keras functional APIs, project the last dimension of the tensor to\n",
    "  # match the new filter size\n",
    "  if out.shape[-1] != input.shape[-1]:\n",
    "    res = Project(out.shape[-1])(res)\n",
    "\n",
    "  return layers.add([res, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeVideo(keras.layers.Layer):\n",
    "  def __init__(self, height, width):\n",
    "    super().__init__()\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "    self.resizing_layer = layers.Resizing(self.height, self.width)\n",
    "\n",
    "  def call(self, video):\n",
    "    \"\"\"\n",
    "      Use the einops library to resize the tensor.  \n",
    "\n",
    "      Args:\n",
    "        video: Tensor representation of the video, in the form of a set of frames.\n",
    "\n",
    "      Return:\n",
    "        A downsampled size of the video according to the new height and width it should be resized to.\n",
    "    \"\"\"\n",
    "    # b stands for batch size, t stands for time, h stands for height, \n",
    "    # w stands for width, and c stands for the number of channels.\n",
    "    old_shape = einops.parse_shape(video, 'b t h w c')\n",
    "    images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n",
    "    images = self.resizing_layer(images)\n",
    "    videos = einops.rearrange(\n",
    "        images, '(b t) h w c -> b t h w c',\n",
    "        t = old_shape['t'])\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\calcolatore\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:192: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_shape = (None, MAX_FRAMES, HEIGHT, WIDTH, 3)\n",
    "input = layers.Input(shape=(input_shape[1:]))\n",
    "x = input\n",
    "\n",
    "x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = ResizeVideo(HEIGHT // 2, WIDTH // 2)(x)\n",
    "\n",
    "# Block 1\n",
    "x = add_residual_block(x, 16, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 4, WIDTH // 4)(x)\n",
    "\n",
    "# Block 2\n",
    "x = add_residual_block(x, 32, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 8, WIDTH // 8)(x)\n",
    "\n",
    "# Block 3\n",
    "x = add_residual_block(x, 64, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 16, WIDTH // 16)(x)\n",
    "\n",
    "# Block 4\n",
    "x = add_residual_block(x, 128, (3, 3, 3))\n",
    "\n",
    "x = layers.GlobalAveragePooling3D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_16\" is incompatible with the layer: expected shape=(None, 100, 18, 18, 3), found shape=(None, 100, 96, 96, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \n\u001b[0;32m      2\u001b[0m               optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m), \n\u001b[0;32m      3\u001b[0m               metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x \u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(X_train, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32), y \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(Y_train, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m      6\u001b[0m                     epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"functional_16\" is incompatible with the layer: expected shape=(None, 100, 18, 18, 3), found shape=(None, 100, 96, 96, 3)"
     ]
    }
   ],
   "source": [
    "model.compile(loss = keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              optimizer = keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(x =tf.convert_to_tensor(X_train, dtype=tf.float32), y = tf.convert_to_tensor(Y_train, dtype=tf.float32),\n",
    "                    epochs = 5, validation_split = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
